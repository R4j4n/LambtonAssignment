{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from typing import Iterable\n",
    "# from nltk.corpus import stopwords   \n",
    "# from spellchecker import SpellChecker\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "    def word_tokenizer(self, text) -> Iterable:\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "class TextCleaner(Tokenizer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # the acronyms url\n",
    "        self._acronyms_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json\"\n",
    "\n",
    "        # link to data where contractios list is present\n",
    "        self._contractions_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json\"\n",
    "\n",
    "        # load the acronym dict\n",
    "        self._acronyms_dict = self.load_acronym()\n",
    "        # load acronym list\n",
    "        self._acronym_list = list(self._acronyms_dict.keys())\n",
    "\n",
    "        # load the contractions dict\n",
    "        self._contractions_dict = self.load_contractions()\n",
    "        # load contractions list\n",
    "        self._contractions_list = list(self._contractions_dict.keys())\n",
    "\n",
    "    def load_acronym(self):\n",
    "        return pd.read_json(self._acronyms_url, typ=\"series\")\n",
    "\n",
    "    def load_contractions(self):\n",
    "        return pd.read_json(self._contractions_url, typ=\"series\")\n",
    "\n",
    "    # Converting to lowercase\n",
    "    def convert_to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    # remove whitespace from the text\n",
    "    def remove_whitespace(self, text):\n",
    "        return text.strip()\n",
    "\n",
    "    # Removing punctuations from the given string\n",
    "    def remove_punctuation(self, text):\n",
    "        # get all the punctuations\n",
    "        punct_str = string.punctuation\n",
    "\n",
    "        # the apostrophe will be remove using contraction.\n",
    "        punct_str = punct_str.replace(\"'\", \"\")\n",
    "        return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "    # Remove any HTML if present in the text.\n",
    "    def remove_html(self, text):\n",
    "        html = re.compile(r\"<.*?>\")\n",
    "        return html.sub(r\"\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    def remove_http(self, text):\n",
    "        http = \"https?://\\S+|www\\.\\S+\"  # matching strings beginning with http (but not just \"http\")\n",
    "        pattern = r\"({})\".format(http)  # creating pattern\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    # Remove any Emojis present in the text.\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "    def convert_acronyms(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._acronym_list:\n",
    "                words = words + self._acronyms_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def convert_contractions(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._contractions_list:\n",
    "                words = words + self._contractions_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = self.convert_to_lowercase(text=text)\n",
    "        text = self.remove_whitespace(text=text)\n",
    "        text = self.remove_punctuation(text=text)\n",
    "        text = self.remove_html(text=text)\n",
    "        text = self.remove_http(text=text)\n",
    "        text = self.remove_emoji(text=text)\n",
    "        text = self.convert_acronyms(text=text)\n",
    "        text = self.convert_contractions(text=text)\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "/tmp/ipykernel_18827/145504393.py:8: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from typing import Iterable\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "    def word_tokenizer(self, text) -> Iterable:\n",
    "        return self.tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"It's so hot in Toronto, isn't it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's\", 'so', 'hot', 'in', 'Toronto', \"isn't\", 'it']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_tokenizer(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
