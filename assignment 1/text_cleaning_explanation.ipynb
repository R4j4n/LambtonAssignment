{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:73: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:73: SyntaxWarning: invalid escape sequence '\\S'\n",
      "/tmp/ipykernel_101805/995558405.py:16: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
      "/tmp/ipykernel_101805/995558405.py:73: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  http = \"https?://\\S+|www\\.\\S+\"  # matching strings beginning with http (but not just \"http\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from typing import Iterable\n",
    "from nltk.corpus import stopwords\n",
    "# from spellchecker import SpellChecker\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "    def word_tokenizer(self, text) -> Iterable:\n",
    "        return self.tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "class TextCleaner(Tokenizer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # the acronyms url\n",
    "        self._acronyms_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json\"\n",
    "\n",
    "        # link to data where contractios list is present\n",
    "        self._contractions_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json\"\n",
    "\n",
    "        # load the acronym dict\n",
    "        self._acronyms_dict = self.load_acronym()\n",
    "        # load acronym list\n",
    "        self._acronym_list = list(self._acronyms_dict.keys())\n",
    "\n",
    "        # load the contractions dict\n",
    "        self._contractions_dict = self.load_contractions()\n",
    "        # load contractions list\n",
    "        self._contractions_list = list(self._contractions_dict.keys())\n",
    "\n",
    "    def load_acronym(self):\n",
    "        return pd.read_json(self._acronyms_url, typ=\"series\")\n",
    "\n",
    "    def load_contractions(self):\n",
    "        return pd.read_json(self._contractions_url, typ=\"series\")\n",
    "\n",
    "    # Converting to lowercase\n",
    "    def convert_to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    # remove whitespace from the text\n",
    "    def remove_whitespace(self, text):\n",
    "        return text.strip()\n",
    "\n",
    "    # Removing punctuations from the given string\n",
    "    def remove_punctuation(self, text):\n",
    "        # get all the punctuations\n",
    "        punct_str = string.punctuation\n",
    "\n",
    "        # the apostrophe will be remove using contraction.\n",
    "        punct_str = punct_str.replace(\"'\", \"\")\n",
    "        return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "    # Remove any HTML if present in the text.\n",
    "    def remove_html(self, text):\n",
    "        html = re.compile(r\"<.*?>\")\n",
    "        return html.sub(r\"\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    def remove_http(self, text):\n",
    "        http = \"https?://\\S+|www\\.\\S+\"  # matching strings beginning with http (but not just \"http\")\n",
    "        pattern = r\"({})\".format(http)  # creating pattern\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    # Remove any Emojis present in the text.\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "    def convert_acronyms(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._acronym_list:\n",
    "                words = words + self._acronyms_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def convert_contractions(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._contractions_list:\n",
    "                words = words + self._contractions_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = self.convert_to_lowercase(text=text)\n",
    "        text = self.remove_whitespace(text=text)\n",
    "        text = self.remove_punctuation(text=text)\n",
    "        text = self.remove_html(text=text)\n",
    "        text = self.remove_http(text=text)\n",
    "        text = self.remove_emoji(text=text)\n",
    "        text = self.convert_acronyms(text=text)\n",
    "        text = self.convert_contractions(text=text)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "prepositions = [\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"across\",\n",
    "    \"after\",\n",
    "    \"against\",\n",
    "    \"among\",\n",
    "    \"around\",\n",
    "    \"at\",\n",
    "    \"before\",\n",
    "    \"behind\",\n",
    "    \"below\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"by\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"in\",\n",
    "    \"inside\",\n",
    "    \"into\",\n",
    "    \"near\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"out\",\n",
    "    \"over\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"toward\",\n",
    "    \"under\",\n",
    "    \"up\",\n",
    "    \"with\",\n",
    "]\n",
    "prepositions_less_common = [\n",
    "    \"aboard\",\n",
    "    \"along\",\n",
    "    \"amid\",\n",
    "    \"as\",\n",
    "    \"beneath\",\n",
    "    \"beyond\",\n",
    "    \"but\",\n",
    "    \"concerning\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"except\",\n",
    "    \"following\",\n",
    "    \"like\",\n",
    "    \"minus\",\n",
    "    \"onto\",\n",
    "    \"outside\",\n",
    "    \"per\",\n",
    "    \"plus\",\n",
    "    \"regarding\",\n",
    "    \"round\",\n",
    "    \"since\",\n",
    "    \"than\",\n",
    "    \"till\",\n",
    "    \"underneath\",\n",
    "    \"unlike\",\n",
    "    \"until\",\n",
    "    \"upon\",\n",
    "    \"versus\",\n",
    "    \"via\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\n",
    "    \"both\",\n",
    "    \"and\",\n",
    "    \"either\",\n",
    "    \"or\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"only\",\n",
    "    \"but\",\n",
    "    \"whether\",\n",
    "    \"or\",\n",
    "]\n",
    "subordinating_conjunctions = [\n",
    "    \"after\",\n",
    "    \"although\",\n",
    "    \"as\",\n",
    "    \"as if\",\n",
    "    \"as long as\",\n",
    "    \"as much as\",\n",
    "    \"as soon as\",\n",
    "    \"as though\",\n",
    "    \"because\",\n",
    "    \"before\",\n",
    "    \"by the time\",\n",
    "    \"even if\",\n",
    "    \"even though\",\n",
    "    \"if\",\n",
    "    \"in order that\",\n",
    "    \"in case\",\n",
    "    \"in the event that\",\n",
    "    \"lest\",\n",
    "    \"now that\",\n",
    "    \"once\",\n",
    "    \"only\",\n",
    "    \"only if\",\n",
    "    \"provided that\",\n",
    "    \"since\",\n",
    "    \"so\",\n",
    "    \"supposing\",\n",
    "    \"that\",\n",
    "    \"than\",\n",
    "    \"though\",\n",
    "    \"till\",\n",
    "    \"unless\",\n",
    "    \"until\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"where\",\n",
    "    \"whereas\",\n",
    "    \"wherever\",\n",
    "    \"whether or not\",\n",
    "    \"while\",\n",
    "]\n",
    "\n",
    "\n",
    "class TextPreprocess(Tokenizer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # initialize the stop words\n",
    "        self.stopwords = stopwords.words(\"english\")\n",
    "\n",
    "        # update stop words\n",
    "        self.stopwords = self.stopwords + [\n",
    "            \"among\",\n",
    "            \"onto\",\n",
    "            \"shall\",\n",
    "            \"thrice\",\n",
    "            \"thus\",\n",
    "            \"twice\",\n",
    "            \"unto\",\n",
    "            \"us\",\n",
    "            \"would\",\n",
    "        ]\n",
    "\n",
    "        # spell checker\n",
    "        self.spell = None\n",
    "\n",
    "        # stemmer object\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "        # spacy lemittizer object\n",
    "        self.spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "        # remove additional stop words\n",
    "        self.additioanal_stop_words = (\n",
    "            prepositions\n",
    "            + prepositions_less_common\n",
    "            + coordinating_conjunctions\n",
    "            + correlative_conjunctions\n",
    "        )\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join(\n",
    "            [word for word in self.word_tokenizer(text) if word not in self.stopwords]\n",
    "        )\n",
    "\n",
    "    def pyspellchecker(self, text):\n",
    "        word_list = self.word_tokenizer(text)\n",
    "        word_list_corrected = []\n",
    "        for word in word_list:\n",
    "            if word in self.spell.unknown(word_list):\n",
    "                word_corrected = self.spell.correction(word)\n",
    "                if word_corrected == None:\n",
    "                    word_list_corrected.append(word)\n",
    "                else:\n",
    "                    word_list_corrected.append(word_corrected)\n",
    "            else:\n",
    "                word_list_corrected.append(word)\n",
    "        text_corrected = \" \".join(word_list_corrected)\n",
    "        return text_corrected\n",
    "\n",
    "    def porter_stemmer(self, text):\n",
    "        text_stem = \" \".join(\n",
    "            [self.stemmer.stem(word) for word in self.word_tokenizer(text)]\n",
    "        )\n",
    "        return text_stem\n",
    "\n",
    "    def lemmatizer(self, text):\n",
    "        text_spacy = \" \".join([token.lemma_ for token in self.spacy_lemmatizer(text)])\n",
    "        return text_spacy\n",
    "\n",
    "    def remove_additional_stopwords(self, text):\n",
    "        return \" \".join(\n",
    "            [\n",
    "                word\n",
    "                for word in self.word_tokenizer(text)\n",
    "                if word not in self.additioanal_stop_words\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, text):\n",
    "\n",
    "        text = self.remove_stopwords(text=text)\n",
    "        # text = self.pyspellchecker(text=text)\n",
    "        text = self.porter_stemmer(text=text)\n",
    "        text = self.lemmatizer(text=text)\n",
    "        text = self.remove_additional_stopwords(text=text)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = TextCleaner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3723369933.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[40], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    text = self.remove_whitespace(text=text)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"BTW the weather isn't ideal in <b>Toronto</b> ðŸ˜Š. isn't it?. Read this news for more info www.google.com/search?q=weather+in+toronto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : BTW the weather isn't ideal in <b>Toronto</b> ðŸ˜Š. isn't it?. Read this news for more info www.google.com/search?q=weather+in+toronto\n",
      "Output : btw the weather isn't ideal in <b>toronto</b> ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.convert_to_lowercase(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in <b>toronto</b> ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n",
      "Output : btw the weather isn't ideal in <b>toronto</b> ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.remove_whitespace(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in <b>toronto</b> ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n",
      "Output : btw the weather isn't ideal in toronto ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.remove_html(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in toronto ðŸ˜Š. isn't it?. read this news for more info www.google.com/search?q=weather+in+toronto\n",
      "Output : btw the weather isn't ideal in toronto ðŸ˜Š. isn't it?. read this news for more info \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.remove_http(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in toronto ðŸ˜Š. isn't it?. read this news for more info \n",
      "Output : btw the weather isn't ideal in toronto ðŸ˜Š isn't it read this news for more info \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.remove_punctuation(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in toronto ðŸ˜Š isn't it read this news for more info \n",
      "Output : btw the weather isn't ideal in toronto  isn't it read this news for more info \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.remove_emoji(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : btw the weather isn't ideal in toronto  isn't it read this news for more info \n",
      "Output : by the way the weather isn't ideal in toronto isn't it read this news for more info\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.convert_acronyms(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : by the way the weather isn't ideal in toronto isn't it read this news for more info\n",
      "Output : by the way the weather is not ideal in toronto is not it read this news for more info\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = txt.convert_contractions(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## class Text Preprocess\n",
    "tp = TextPreprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : by the way the weather is not ideal in toronto is not it read this news for more info\n",
      "Output : way weather ideal toronto read news info\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = tp.remove_stopwords(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : The temperature is more hotter than it used to be. How are you planning to deal with rising universal temperature.\n",
      "Output : the temperatur is more hotter than it use to be how are you plan to deal with rise univers temperatur\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"The temperature is more hotter than it used to be. How are you planning to deal with rising universal temperature.\"\n",
    "\n",
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = tp.porter_stemmer(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : the temperatur is more hotter than it use to be how are you plan to deal with rise univers temperatur\n",
      "Output : the temperatur be more hot than it use to be how be you plan to deal with rise univer temperatur\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = tp.lemmatizer(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : the temperatur be more hot than it use to be how be you plan to deal with rise univer temperatur\n",
      "Output : the temperatur be more hot it use be how be you plan deal rise univer temperatur\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input : {input_sentence}\")\n",
    "input_sentence = tp.remove_additional_stopwords(input_sentence)\n",
    "print(f\"Output : {input_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
