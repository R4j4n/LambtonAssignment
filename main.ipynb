{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.offline import *\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database \n",
    "conn = sqlite3.connect(\"kijiji_real_estate_gta.db\", isolation_level=None,\n",
    "                       detect_types=sqlite3.PARSE_COLNAMES)\n",
    "\n",
    "# convert the database into a pandas data frame.\n",
    "df = pd.read_sql_query(\"SELECT * FROM properties\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As database is storing null value as N/A converting N/A to null value, replacing 'N/A' with np.nan\n",
    "df.replace('N/A', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingness Analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all columns with missing values.\n",
    "\n",
    "col_mis_val = df.columns[df.isnull().sum() > 0]\n",
    "col_mis_val = col_mis_val.to_list()\n",
    "\n",
    "# visualize missing values with missingno.\n",
    "\n",
    "msno.matrix(df[col_mis_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Completeness per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(df[col_mis_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns whose missing values are greater than 50%\n",
    "to_be_dropeed = df[col_mis_val].columns[df[col_mis_val].isna().mean() * 100 > 50]\n",
    "print(f\"More than 50% missing values:\\n{list(to_be_dropeed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with missing address \n",
    "df.dropna(subset=['address'], inplace=True)\n",
    "\n",
    "print(f\"Missing Values in address : {df.address.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing at Random Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check of pets_friendly \n",
    "df[\"pets_friendly\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that data consists of only `Yes` records, so `No` record is intentionally left blank. We can add `No` to all remaining null rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pets_friendly\"] = df[\"pets_friendly\"].fillna(\"No\")\n",
    "df[\"pets_friendly\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting price to proper format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Please Contact' with np.nan in the price column as some listing contains please contact info. \n",
    "df['price'].replace('Please Contact', np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN in the price column\n",
    "df.dropna(subset=['price'], inplace=True)\n",
    "\n",
    "# Remove non-numeric characters from the price and convert to float, trying to remove $\n",
    "df['price'] = df['price'].str.replace(r'[^0-9.]', '', regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_df = deepcopy(df)\n",
    "# fe_df.columns\n",
    "\n",
    "# fe_df = fe_df[[\"price\"]]\n",
    "\n",
    "\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "# import warnings \n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# def impute_categorical_missing_data(passed_col):\n",
    "    \n",
    "#     df_null = df[df[passed_col].isnull()]\n",
    "#     df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "#     X = df_not_null.drop(passed_col, axis=1)\n",
    "#     y = df_not_null[passed_col]\n",
    "    \n",
    "#     other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "#     label_encoder = LabelEncoder()\n",
    "\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "#             X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "#     if passed_col in bool_cols:\n",
    "#         y = label_encoder.fit_transform(y)\n",
    "        \n",
    "#     iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "#     for col in other_missing_cols:\n",
    "#         if X[col].isnull().sum() > 0:\n",
    "#             col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "#             imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "#             X[col] = imputed_values[:, 0]\n",
    "#         else:\n",
    "#             pass\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     rf_classifier = RandomForestClassifier()\n",
    "\n",
    "#     rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "#     acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#     print(\"The feature '\"+ passed_col+ \"' has been imputed with\", round((acc_score * 100), 2), \"accuracy\\n\")\n",
    "\n",
    "#     X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "#             X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "#     for col in other_missing_cols:\n",
    "#         if X[col].isnull().sum() > 0:\n",
    "#             col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "#             imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "#             X[col] = imputed_values[:, 0]\n",
    "#         else:\n",
    "#             pass\n",
    "                \n",
    "#     if len(df_null) > 0: \n",
    "#         df_null[passed_col] = rf_classifier.predict(X)\n",
    "#         if passed_col in bool_cols:\n",
    "#             df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})\n",
    "#         else:\n",
    "#             pass\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "#     return df_combined[passed_col]\n",
    "\n",
    "# def impute_continuous_missing_data(passed_col):\n",
    "    \n",
    "#     df_null = df[df[passed_col].isnull()]\n",
    "#     df_not_null = df[df[passed_col].notnull()]\n",
    "\n",
    "#     X = df_not_null.drop(passed_col, axis=1)\n",
    "#     y = df_not_null[passed_col]\n",
    "    \n",
    "#     other_missing_cols = [col for col in missing_data_cols if col != passed_col]\n",
    "    \n",
    "#     label_encoder = LabelEncoder()\n",
    "\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "#             X[col] = label_encoder.fit_transform(X[col])\n",
    "    \n",
    "#     iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)\n",
    "\n",
    "#     for col in other_missing_cols:\n",
    "#         if X[col].isnull().sum() > 0:\n",
    "#             col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "#             imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "#             X[col] = imputed_values[:, 0]\n",
    "#         else:\n",
    "#             pass\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     rf_regressor = RandomForestRegressor()\n",
    "\n",
    "#     rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "#     print(\"MAE =\", mean_absolute_error(y_test, y_pred), \"\\n\")\n",
    "\n",
    "#     X = df_null.drop(passed_col, axis=1)\n",
    "\n",
    "#     for col in X.columns:\n",
    "#         if X[col].dtype == 'object' or X[col].dtype == 'category':\n",
    "#             X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "#     for col in other_missing_cols:\n",
    "#         if X[col].isnull().sum() > 0:\n",
    "#             col_with_missing_values = X[col].values.reshape(-1, 1)\n",
    "#             imputed_values = iterative_imputer.fit_transform(col_with_missing_values)\n",
    "#             X[col] = imputed_values[:, 0]\n",
    "#         else:\n",
    "#             pass\n",
    "                \n",
    "#     if len(df_null) > 0: \n",
    "#         df_null[passed_col] = rf_regressor.predict(X)\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "#     df_combined = pd.concat([df_not_null, df_null])\n",
    "    \n",
    "#     return df_combined[passed_col]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting address to longitude and latitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pgeocode\n",
    "import pandas as pd\n",
    "from postalcodes_ca import fsa_codes\n",
    "\n",
    "class PostalCodeProcessor:\n",
    "    def __init__(self, df, address_column):\n",
    "        self.df = df\n",
    "        self.address_column = address_column\n",
    "        self.nomi = pgeocode.Nominatim('CA')\n",
    "        self.fsa_codes = fsa_codes\n",
    "\n",
    "    def extract_postal_codes(self, address):\n",
    "        pattern = r'\\b[A-Z]\\d[A-Z][ -]?\\d[A-Z]\\d\\b'\n",
    "        match = re.search(pattern, address)\n",
    "        return match.group() if match else None\n",
    "\n",
    "    def format_canadian_postal_code(self, postal_code):\n",
    "        pattern = r'^([A-Z]\\d[A-Z])?(\\s)?(\\d[A-Z]\\d)$'\n",
    "        match = re.search(pattern, postal_code.upper())\n",
    "        if match:\n",
    "            return f\"{match.group(1)} {match.group(3)}\" if match.group(2) != '\\s' else postal_code.upper()\n",
    "        return None\n",
    "\n",
    "    def process_dataframe(self):\n",
    "        result = []\n",
    "        for address in self.df[self.address_column]:\n",
    "            postal_code = self.extract_postal_codes(address)\n",
    "            if postal_code:\n",
    "                formatted_postal_code = self.format_canadian_postal_code(postal_code)\n",
    "                fsa = formatted_postal_code.split()[0]\n",
    "\n",
    "                try:\n",
    "\n",
    "                    fsa_result = self.fsa_codes[fsa]\n",
    "                    city = fsa_result.name.split(\"(\")[0].strip()\n",
    "                    query = self.nomi.query_postal_code(formatted_postal_code)\n",
    "                    lat, lon = query[\"latitude\"], query[\"longitude\"]\n",
    "                    result.append([lat, lon, city])\n",
    "\n",
    "                except Exception as e:\n",
    "                    result.append([None, None, None])\n",
    "\n",
    "            else:\n",
    "                result.append([None, None, None])\n",
    "        \n",
    "        # Add new columns to the dataframe\n",
    "        self.df['latitude'] = [r[0] for r in result]\n",
    "        self.df['longitude'] = [r[1] for r in result]\n",
    "        self.df['city'] = [r[2] for r in result]\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "processor = PostalCodeProcessor(df, \"address\")\n",
    "\n",
    "# add longitude, latitude and city in dataframe\n",
    "df = processor.process_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows with either of the missing values.\n",
    "df.dropna(subset=['latitude', 'longitude', 'city'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the price columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot for the price column\n",
    "fig = px.box(df, y='price', title='Box Plot of Listing Prices', labels={'price': 'Price'}, template='plotly_dark')\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.90)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers\n",
    "df = df[(df['price'] >= (Q1 - 1.5 * IQR)) & (df['price'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "\n",
    "fig = px.box(df, y='price', title='Box Plot of Listing Prices', labels={'price': 'Price'}, template='plotly_dark')\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the listing distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = df[\"latitude\"].mean()\n",
    "longitude = df[\"longitude\"].mean()\n",
    "map = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    folium.Marker(location=[row[\"latitude\"], row[\"longitude\"]], \n",
    "                  icon=folium.Icon(color='black',icon_color='#FF0000')).add_to(marker_cluster)\n",
    "\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 city with highest listings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get listings per city\n",
    "listing_per_city = df.city.value_counts().to_dict()\n",
    "\n",
    "# convert dict to list\n",
    "city = list(listing_per_city.keys())[:10]\n",
    "count = list(listing_per_city.values())[:10]\n",
    "\n",
    "\n",
    "fig = px.bar(x=city, y=count, text=count, template='plotly_dark')\n",
    "# fig.update_traces(texttemplate=\"%{y}\", textposition=\"auto\")\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Top 10 cities with property listing.',\n",
    "    xaxis_title='city',\n",
    "    yaxis_title='Number of listings.',\n",
    "    xaxis={'categoryorder':'total descending'},\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Average price per city**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price_per_city = df.groupby('city')['price'].mean().reset_index()\n",
    "\n",
    "avg_price_per_city = avg_price_per_city.sort_values(by='price', ascending=False)[:10]\n",
    "\n",
    "fig = px.bar(avg_price_per_city, x='city', y='price', title='Average Listing Price per City', labels={'price': 'Average Price', 'city': 'City'},text=avg_price_per_city[\"price\"], template='plotly_dark')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Average price in top 10 cities.',\n",
    "    xaxis_title='City',\n",
    "    yaxis_title='Average Listing price.',\n",
    "    xaxis={'categoryorder':'total descending'},\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap of price distribution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats_longs_weight = [[x[-1][\"latitude\"],x[-1][\"longitude\"],x[-1][\"price\"]] for x in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_obj = folium.Map(location=[latitude, longitude] , zoom_start = 9)\n",
    "\n",
    "HeatMap(lats_longs_weight).add_to(map_obj)\n",
    "\n",
    "map_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
